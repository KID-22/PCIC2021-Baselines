{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline2: MF_Naive MF_IPS CausE (Using extract data from Baseline1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have extracted deterministic data according to the problem description on Baseline 1, which can be used for further mining with some more complicated model. Here, we will introduce three model, hoping to inspire you, especially from the perspective of causality. For more details, you can find the whole project from https://github.com/KID-22/PCIC-2021-Baselines. Welcome to watch, star and fork! Note that some new baselines will update soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see more detailed information about these models from paper as follows:  \n",
    "[1] Koren et al. 2009. Matrix factorization techniques for recommender systems. In Computer.  \n",
    "[2] Schnabel et al. 2016. Recommendations as Treatments: Debiasing Learning and Evaluation. In JMLR.  \n",
    "[3] Bonner et al. 2018. Causal embeddings for recommendation. In RecSys.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models were initially used in the rating prediction task. Here, the task of our problem is to predict whether a user like(1) or dislike(0) a tag, which can be considered as a binary ratings prediction task. So we first use the deterministic data extracting from Baseline 1 to constuct the user-item interaction matrix. Then, we use these rating model to predict the user performance.  \n",
    "**Note that we haven't used the rating.txt, and how to use this dataset is left to you participants to explore.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "#### MF_Naive.py\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.init import normal_\n",
    "\n",
    "\n",
    "class MF_Naive(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size, device='cpu'):\n",
    "        super(MF_Naive, self).__init__()\n",
    "\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "\n",
    "        self.user_e = nn.Embedding(self.num_users, embedding_size)\n",
    "        self.item_e = nn.Embedding(self.num_items, embedding_size)\n",
    "        self.user_b = nn.Embedding(self.num_users, 1)\n",
    "        self.item_b = nn.Embedding(self.num_items, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            normal_(module.weight.data, mean=0.0, std=0.1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_embedding = self.user_e(user)\n",
    "        item_embedding = self.item_e(item)\n",
    "\n",
    "        preds = self.user_b(user)\n",
    "        preds += self.item_b(item)\n",
    "        preds += (user_embedding * item_embedding).sum(dim=1, keepdim=True)\n",
    "\n",
    "        return preds.squeeze()\n",
    "\n",
    "    def calculate_loss(self, user_list, item_list, label_list):\n",
    "        return self.loss(self.forward(user_list, item_list), label_list)\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)\n",
    "\n",
    "    def get_optimizer(self, lr, weight_decay):\n",
    "        return torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def get_embedding(self):\n",
    "        return self.user_e, self.item_e\n",
    "\n",
    "```\n",
    "#### MF_IPS.py\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.init import normal_\n",
    "from .loss import IPSLoss\n",
    "\n",
    "\n",
    "class MF_IPS(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size, inverse_propensity, device):\n",
    "        super(MF_IPS, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.inverse_propensity = inverse_propensity\n",
    "\n",
    "        self.user_e = nn.Embedding(self.num_users, embedding_size)\n",
    "        self.item_e = nn.Embedding(self.num_items, embedding_size)\n",
    "        self.user_b = nn.Embedding(self.num_users, 1)\n",
    "        self.item_b = nn.Embedding(self.num_items, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        self.loss = IPSLoss(device)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            normal_(module.weight.data, mean=0.0, std=0.1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_embedding = self.user_e(user)\n",
    "        item_embedding = self.item_e(item)\n",
    "\n",
    "        preds = self.user_b(user)\n",
    "        preds += self.item_b(item)\n",
    "        preds += (user_embedding * item_embedding).sum(dim=1, keepdim=True)\n",
    "\n",
    "        return preds.squeeze()\n",
    "\n",
    "    def calculate_loss(self, user_list, item_list, label_list):\n",
    "        return self.loss(self.forward(user_list, item_list), label_list, self.inverse_propensity, item_list)\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)\n",
    "\n",
    "    def get_optimizer(self, lr, weight_decay):\n",
    "        return torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def get_embedding(self):\n",
    "        return self.user_e, self.item_e\n",
    "```\n",
    "#### CausE.py\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.init import normal_\n",
    "\n",
    "\n",
    "class CausE(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size,\n",
    "                 reg_c, reg_t, reg_tc, s_c, s_t, device='cpu'):\n",
    "        super(CausE, self).__init__()\n",
    "        self.user_e = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_e_c = nn.Embedding(num_items, embedding_size)\n",
    "        self.item_e_t = nn.Embedding(num_items, embedding_size)\n",
    "        self.user_b = nn.Embedding(num_users, 1)\n",
    "        self.item_b = nn.Embedding(num_items, 1)\n",
    "        self.reg_c = reg_c\n",
    "        self.reg_t = reg_t\n",
    "        self.reg_tc = reg_tc\n",
    "        self.s_c = s_c\n",
    "        self.s_t = s_t\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        self.loss_c = nn.MSELoss()\n",
    "        self.loss_t = nn.MSELoss()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            normal_(module.weight.data, mean=0.0, std=0.1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_embedding = self.user_e(user)\n",
    "        item_embedding = self.item_e_c(item)\n",
    "\n",
    "        preds = self.user_b(user)\n",
    "        preds += self.item_b(item)\n",
    "        preds += (user_embedding * item_embedding).sum(dim=1, keepdim=True)\n",
    "        return preds.squeeze()\n",
    "\n",
    "    def calculate_loss(self, user_list, item_list, label_list, control):\n",
    "        user_embedding = self.user_e(user_list)\n",
    "\n",
    "        item_embedding_c = self.item_e_c(item_list)\n",
    "        item_embedding_t = self.item_e_t(item_list)\n",
    "\n",
    "        dot_c = (user_embedding * item_embedding_c).sum(dim=1, keepdim=True)\n",
    "        pred_c = dot_c + self.user_b(user_list) + self.item_b(item_list)\n",
    "        pred_c = pred_c.squeeze()\n",
    "        dot_t = (user_embedding * item_embedding_t).sum(dim=1, keepdim=True)\n",
    "        pred_t = dot_t + self.user_b(user_list) + self.item_b(item_list)\n",
    "        pred_t = pred_t.squeeze()\n",
    "\n",
    "        loss = self.loss_c(pred_c, label_list)\n",
    "        loss += self.loss_t(pred_t, label_list)\n",
    "        loss_reg_tc = self.reg_tc * torch.norm(item_embedding_c - item_embedding_t, 2)\n",
    "        return loss + loss_reg_tc\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)\n",
    "\n",
    "    def get_optimizer(self, lr, weight_decay):\n",
    "        return torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main.py\n",
    "```python\n",
    "from config import opt\n",
    "import os\n",
    "import models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from metrics import AUC\n",
    "from utils import MF_DATA, CausE_DATA, evaluate_model\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "seed_num = 2021\n",
    "print(\"seed_num:\", seed_num)\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(seed_num)\n",
    "\n",
    "\n",
    "# propensity estimation for MF_IPS\n",
    "def cal_propensity_score():\n",
    "    ps_train_data = np.loadtxt(opt.ps_train_data)\n",
    "    ps_train_data = ps_train_data.astype(int)\n",
    "    ps_val_data = np.loadtxt(opt.ps_val_data)\n",
    "    ps_val_data = ps_val_data.astype(int)\n",
    "\n",
    "    user_num = 1000\n",
    "    item_num = 1720\n",
    "    P_L_TO = np.bincount(ps_train_data[:, 2], minlength=2)[:]\n",
    "    tmp = P_L_TO.sum()\n",
    "    P_L_TO = P_L_TO / P_L_TO.sum()\n",
    "\n",
    "    P_L_T = np.bincount(ps_val_data[:, 2], minlength=2)[:]\n",
    "    P_L_T = P_L_T / P_L_T.sum()\n",
    "\n",
    "    P_O_T = tmp / (user_num * item_num)\n",
    "    P = P_L_TO * P_O_T / P_L_T\n",
    "\n",
    "    propensity_score = [P] * item_num\n",
    "\n",
    "    return propensity_score\n",
    "\n",
    "\n",
    "# train for CausE\n",
    "def train_CausE():\n",
    "    train_data = CausE_DATA(opt.s_c_data, opt.s_t_data)\n",
    "    val_data = MF_DATA(opt.cause_val_data)\n",
    "    train_dataloader_s_c = DataLoader(train_data.s_c,\n",
    "                                      opt.batch_size,\n",
    "                                      shuffle=True)\n",
    "    train_dataloader_s_t = DataLoader(train_data.s_t,\n",
    "                                      opt.batch_size,\n",
    "                                      shuffle=True)\n",
    "    model = getattr(models,\n",
    "                    opt.model)(train_data.users_num, train_data.items_num,\n",
    "                               opt.embedding_size, opt.reg_c, opt.reg_c,\n",
    "                               opt.reg_tc, train_data.s_c[:, :2].tolist(),\n",
    "                               train_data.s_t[:, :2].tolist())\n",
    "\n",
    "    model.to(opt.device)\n",
    "    optimizer = model.get_optimizer(opt.lr, opt.weight_decay)\n",
    "\n",
    "    best_mse = 10000000.\n",
    "    best_mae = 10000000.\n",
    "    best_auc = 0\n",
    "    best_iter = 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(opt.max_epoch):\n",
    "        t1 = time()\n",
    "        for i, data in tqdm(enumerate(train_dataloader_s_c)):\n",
    "            # train model\n",
    "            user = data[:, 0].to(opt.device)\n",
    "            item = data[:, 1].to(opt.device)\n",
    "            label = data[:, 2].to(opt.device)\n",
    "\n",
    "            loss = model.calculate_loss(user.long(),\n",
    "                                        item.long(),\n",
    "                                        label.float(),\n",
    "                                        control=True)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % opt.verbose == 0:\n",
    "            print('Epoch %d :' % (epoch))\n",
    "            print('s_c Loss = ', loss.item())\n",
    "\n",
    "        for i, data in tqdm(enumerate(train_dataloader_s_t)):\n",
    "            # train model\n",
    "            user = data[:, 0].to(opt.device)\n",
    "            item = data[:, 1].to(opt.device)\n",
    "            label = data[:, 2].to(opt.device)\n",
    "\n",
    "            loss = model.calculate_loss(user.long(),\n",
    "                                        item.long(),\n",
    "                                        label.float(),\n",
    "                                        control=False)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        (mae, mse, rmse, auc) = evaluate_model(model, val_data, opt)\n",
    "\n",
    "        if opt.metric == 'mae':\n",
    "            if mae < best_mae:\n",
    "                best_mae, best_mse, best_auc, best_iter = mae, mse, auc, epoch\n",
    "                torch.save(model.state_dict(), \"./checkpoint/ci-mae-model.pth\")\n",
    "        elif opt.metric == 'mse':\n",
    "            if mse < best_mse:\n",
    "                best_mae, best_mse, best_auc, best_iter = mae, mse, auc, epoch\n",
    "                torch.save(model.state_dict(), \"./checkpoint/ci-mse-model.pth\")\n",
    "        elif opt.metric == 'auc':\n",
    "            if auc > best_auc:\n",
    "                best_mae, best_mse, best_auc, best_iter = mae, mse, auc, epoch\n",
    "                torch.save(model.state_dict(), \"./checkpoint/ci-auc-model.pth\")\n",
    "\n",
    "        if epoch % opt.verbose == 0:\n",
    "            print('s_t Loss = ', loss.item())\n",
    "            print(\n",
    "                'Val MAE = %.4f, MSE = %.4f, RMSE = %.4f, AUC = %.4f [%.1f s]'\n",
    "                % (mae, mse, rmse, auc, time() - t1))\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "    print(\"train end\\nBest Epoch %d:  MAE = %.4f, MSE = %.4f, AUC = %.4f\" %\n",
    "          (best_iter, best_mae, best_mse, best_auc))\n",
    "\n",
    "    best_model = getattr(models,\n",
    "                         opt.model)(train_data.users_num, train_data.items_num,\n",
    "                                    opt.embedding_size, opt.reg_c, opt.reg_c,\n",
    "                                    opt.reg_tc, train_data.s_c[:, :2].tolist(),\n",
    "                                    train_data.s_t[:, :2].tolist())\n",
    "    best_model.to(opt.device)\n",
    "\n",
    "    if opt.metric == 'mae':\n",
    "        best_model.load_state_dict(torch.load(\"./checkpoint/ci-mae-model.pth\"))\n",
    "    elif opt.metric == 'mse':\n",
    "        best_model.load_state_dict(torch.load(\"./checkpoint/ci-mse-model.pth\"))\n",
    "    elif opt.metric == 'auc':\n",
    "        best_model.load_state_dict(torch.load(\"./checkpoint/ci-auc-model.pth\"))\n",
    "\n",
    "    print(\"\\n========================= best model =========================\")\n",
    "    mae, mse, rmse, auc = evaluate_model(best_model, train_data, opt)\n",
    "    print('Train MAE = %.4f, MSE = %.4f, RMSE = %.4f, AUC = %.4f' %\n",
    "          (mae, mse, rmse, auc))\n",
    "    mae, mse, rmse, auc = evaluate_model(best_model, val_data, opt)\n",
    "    print('Val MAE = %.4f, MSE = %.4f, RMSE = %.4f, AUC = %.4f' %\n",
    "          (mae, mse, rmse, auc))\n",
    "    print(\"===============================================================\\n\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# train for MF_Naive and MF_IPS\n",
    "def train(propensity_score):\n",
    "    print('train begin')\n",
    "\n",
    "    train_all_data = MF_DATA(opt.train_data)\n",
    "    train_data = copy.deepcopy(train_all_data)\n",
    "    val_data = MF_DATA(opt.val_all_data)\n",
    "    train_dataloader = DataLoader(train_data, opt.batch_size, shuffle=True)\n",
    "\n",
    "    if opt.model == 'MF_IPS':\n",
    "        inverse_propensity = np.reciprocal(propensity_score)\n",
    "        model = getattr(models, opt.model)(train_all_data.users_num,\n",
    "                                           train_all_data.items_num,\n",
    "                                           opt.embedding_size,\n",
    "                                           inverse_propensity, opt.device)\n",
    "    elif opt.model == 'MF_Naive':\n",
    "        model = getattr(models, opt.model)(train_all_data.users_num,\n",
    "                                           train_all_data.items_num,\n",
    "                                           opt.embedding_size, opt.device)\n",
    "\n",
    "    model.to(opt.device)\n",
    "    optimizer = model.get_optimizer(opt.lr, opt.weight_decay)\n",
    "\n",
    "    best_mse = 10000000.\n",
    "    best_mae = 10000000.\n",
    "    best_auc = 0\n",
    "    best_iter = 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(opt.max_epoch):\n",
    "        t1 = time()\n",
    "        for i, data in tqdm(enumerate(train_dataloader)):\n",
    "            user = data[:, 0].to(opt.device)\n",
    "            item = data[:, 1].to(opt.device)\n",
    "            label = data[:, 2].to(opt.device)\n",
    "\n",
    "            loss = model.calculate_loss(user.long(), item.long(),\n",
    "                                        label.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        (mae, mse, rmse, auc) = evaluate_model(model, val_data, opt)\n",
    "\n",
    "        if opt.metric == 'mae':\n",
    "            if mae < best_mae:\n",
    "                best_mae, best_mse, best_auc, best_iter = mae, mse, auc, epoch\n",
    "                torch.save(model.state_dict(), \"./checkpoint/ci-mae-model.pth\")\n",
    "        elif opt.metric == 'mse':\n",
    "            if mse < best_mse:\n",
    "                best_mae, best_mse, best_auc, best_iter = mae, mse, auc, epoch\n",
    "                torch.save(model.state_dict(), \"./checkpoint/ci-mse-model.pth\")\n",
    "        elif opt.metric == 'auc':\n",
    "            if auc > best_auc:\n",
    "                best_mae, best_mse, best_auc, best_iter = mae, mse, auc, epoch\n",
    "                torch.save(model.state_dict(), \"./checkpoint/ci-auc-model.pth\")\n",
    "\n",
    "        if epoch % opt.verbose == 0:\n",
    "            print('Epoch %d [%.1f s]:', epoch, t2 - t1)\n",
    "            print('Train Loss = ', loss.item())\n",
    "            print(\n",
    "                'Val MAE = %.4f, MSE = %.4f, RMSE = %.4f, AUC = %.4f [%.1f s]'\n",
    "                % (mae, mse, rmse, auc, time() - t2))\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "    print(\"train end\\nBest Epoch %d:  MAE = %.4f, MSE = %.4f, AUC = %.4f\" %\n",
    "          (best_iter, best_mae, best_mse, best_auc))\n",
    "\n",
    "    if opt.model == 'MF_IPS':\n",
    "        inverse_propensity = np.reciprocal(propensity_score)\n",
    "        best_model = getattr(models, opt.model)(train_all_data.users_num,\n",
    "                                                train_all_data.items_num,\n",
    "                                                opt.embedding_size,\n",
    "                                                inverse_propensity, opt.device)\n",
    "    elif opt.model == 'MF_Naive':\n",
    "        best_model = getattr(models, opt.model)(train_all_data.users_num,\n",
    "                                                train_all_data.items_num,\n",
    "                                                opt.embedding_size, opt.device)\n",
    "\n",
    "    best_model.to(opt.device)\n",
    "\n",
    "    if opt.metric == 'mae':\n",
    "        best_model.load_state_dict(torch.load(\"./checkpoint/ci-mae-model.pth\"))\n",
    "    elif opt.metric == 'mse':\n",
    "        best_model.load_state_dict(torch.load(\"./checkpoint/ci-mse-model.pth\"))\n",
    "    elif opt.metric == 'auc':\n",
    "        best_model.load_state_dict(torch.load(\"./checkpoint/ci-auc-model.pth\"))\n",
    "\n",
    "    print(\"\\n========================= best model =========================\")\n",
    "    mae, mse, rmse, auc = evaluate_model(best_model, train_data, opt)\n",
    "    print('Train MAE = %.4f, MSE = %.4f, RMSE = %.4f, AUC = %.4f' %\n",
    "          (mae, mse, rmse, auc))\n",
    "    mae, mse, rmse, auc = evaluate_model(best_model, val_data, opt)\n",
    "    print('Val MAE = %.4f, MSE = %.4f, RMSE = %.4f, AUC = %.4f' %\n",
    "          (mae, mse, rmse, auc))\n",
    "    print(\"===============================================================\\n\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# gengerate submit file\n",
    "def generate_submit(model):\n",
    "    test_data = np.loadtxt(opt.test_data, dtype=int)\n",
    "    user = torch.LongTensor(test_data[:, 0]).to(opt.device)\n",
    "    item = torch.LongTensor(test_data[:, 1]).to(opt.device)\n",
    "    pred = model.predict(user, item).to(opt.device)\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    # normalize\n",
    "    pred = (pred - np.min(pred)) / (np.max(pred) - np.min(pred))\n",
    "    pred = pred.reshape(-1, 1)\n",
    "    submit = np.hstack((test_data, pred))\n",
    "    np.savetxt(\"submit.csv\", submit, fmt=('%d', '%d', '%f'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Demo of argparse\")\n",
    "    parser.add_argument('--model', default='MF_Naive')\n",
    "    parser.add_argument('--batch_size', type=int, default=512)\n",
    "    parser.add_argument('--epoch', type=int, default=50)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--metric',\n",
    "                        default='auc',\n",
    "                        choices=[\"mae\", \"mse\", \"auc\"])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    opt.model = args.model\n",
    "    opt.batch_size = args.batch_size\n",
    "    opt.max_epoch = args.epoch\n",
    "    opt.lr = args.lr\n",
    "    opt.metric = args.metric\n",
    "\n",
    "    print('\\n'.join(['%s:%s' % item for item in opt.__dict__.items()]))\n",
    "\n",
    "    if opt.model == 'MF_IPS' or opt.model == 'MF_Naive':\n",
    "        propensity_score = cal_propensity_score()\n",
    "        propensity_score = np.array(propensity_score).astype(float)\n",
    "        best_model = train(propensity_score)\n",
    "        generate_submit(best_model)\n",
    "    elif opt.model == 'CausE':\n",
    "        best_model = train_CausE()\n",
    "        generate_submit(best_model)\n",
    "\n",
    "    print('end')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are as follow: \n",
    "\n",
    "|  model |  Validation  |  Test  |  \n",
    "| :----: | :----: | :----: |\n",
    "|  MF_Naive  | 0.7346  | 0.6615 \n",
    "|  MF_IPS  | 0.7451  | 0.6798\n",
    "|  CausE  | 0.7448  | 0.6822 \n",
    "\n",
    "We can see that MF with causal methods(MF_IPS and CausE) significantly outperformed MF_Naive. So maybe we can solve this problem more by using some novel and causal algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
